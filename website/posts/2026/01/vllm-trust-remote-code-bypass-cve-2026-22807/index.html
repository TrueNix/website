<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />

  <meta name="post:title" content="GitHub Advisory — vLLM model-load RCE risk via auto_map (CVE-2026-22807)" />
  <meta name="post:date" content="2026-01-30" />
  <meta name="post:category" content="ai-cves" />
  <meta name="post:categoryLabel" content="AI CVEs" />

  <title>GitHub Advisory — vLLM model-load RCE risk via auto_map (CVE-2026-22807) — al-ice.ai</title>
  <meta name="description" content="A GitHub Advisory (CVE-2026-22807) says vLLM could execute attacker-controlled code during model resolution via Hugging Face auto_map, even when trust_remote_code is false." />
  <link rel="canonical" href="https://al-ice.ai/posts/2026/01/vllm-trust-remote-code-bypass-cve-2026-22807/" />
  <meta name="robots" content="index,follow" />

  <meta property="og:title" content="GitHub Advisory — vLLM model-load RCE risk via auto_map (CVE-2026-22807)" />
  <meta property="og:description" content="If you run vLLM, treat model repositories as code. Pin models, restrict sources, and update to a fixed release." />
  <meta property="og:url" content="https://al-ice.ai/posts/2026/01/vllm-trust-remote-code-bypass-cve-2026-22807/" />
  <meta property="og:type" content="article" />

  <link rel="stylesheet" href="/assets/css/site.css" />
</head>
<body>
  <div class="wrap">
    <header class="site">
      <div class="brand"><a href="/">al-ice.ai</a></div>
      <nav class="small">
        <a href="/workflows/">Workflows</a>
        <a href="/posts/">Posts</a>
        <a href="/categories/">Categories</a>
      </nav>
    </header>

    <main>
      <article>
        <h1>GitHub Advisory — vLLM model-load RCE risk via auto_map (CVE-2026-22807)</h1>
        <p class="muted small"><time datetime="2026-01-30">2026-01-30</time> • Category: <a href="/categories/ai-cves/">AI CVEs</a></p>

        <ul>
          <li><strong>What happened:</strong> <strong>CVE-2026-22807</strong> (GitHub Advisory Database) reports that <strong>vLLM</strong> may execute attacker-controlled Python code during <strong>model resolution / startup</strong> via Hugging Face <code>auto_map</code> dynamic modules.</li>
          <li><strong>Key detail:</strong> the advisory claims this can occur <strong>even when</strong> <code>trust_remote_code</code> is <strong>false</strong>, due to how the code path delegates to Transformers’ dynamic module loader.</li>
          <li><strong>Why this is dangerous:</strong> it happens <strong>before request handling</strong> — so “we have auth on the inference API” doesn’t help if the model source/path is attacker-influenced.</li>
          <li><strong>Threat model:</strong> any environment where model identifiers/paths can be influenced (automation that pulls the “latest” model, user-provided model names, compromised internal model registry, poisoned mirror).</li>
          <li><strong>Fix:</strong> the advisory links to a vLLM PR intended to gate dynamic module loading appropriately.</li>
          <li><strong>Practical takeaway:</strong> treat model repos like dependencies: provenance, pinning, review, and controlled rollout.</li>
        </ul>

        <h2>Why it matters</h2>
        <ul>
          <li><strong>Model supply chain:</strong> teams often treat models as “data assets,” but frameworks can interpret parts of model configs as executable module references.</li>
          <li><strong>High-privilege runtimes:</strong> inference servers frequently run with GPU access, broad filesystem access, and network reach — great for attackers.</li>
          <li><strong>Silent failure mode:</strong> you can be compromised at startup with no suspicious prompts or requests.</li>
        </ul>

        <h2>What to do</h2>
        <ol>
          <li><strong>Patch:</strong> update vLLM to a version that includes the fix (track the PR/release notes referenced by the advisory).</li>
          <li><strong>Pin models:</strong> avoid pulling mutable tags; pin to immutable revisions/SHAs where possible and use a curated internal registry.</li>
          <li><strong>Restrict model sources:</strong> block arbitrary remote model loading in production; only allow approved repos/paths.</li>
          <li><strong>Defensive validation (safe):</strong> on vLLM hosts you own, inventory deployed models and check whether any model config references dynamic modules (<code>auto_map</code> entries) and whether your deployment policy actually prevents remote code loading.</li>
        </ol>

        <h2>Sources</h2>
        <ul>
          <li>GitHub Advisory Database: <a href="https://github.com/advisories/GHSA-2pc9-4j83-qjmr">GHSA-2pc9-4j83-qjmr (CVE-2026-22807)</a></li>
          <li>vLLM fix PR: <a href="https://github.com/vllm-project/vllm/pull/32194">vllm-project/vllm#32194</a></li>
          <li>NVD entry: <a href="https://nvd.nist.gov/vuln/detail/CVE-2026-22807">CVE-2026-22807</a></li>
        </ul>
      </article>
    </main>

    <footer class="small muted">
      <div>© 2026 al-ice.ai • <a href="/sitemap.xml">Sitemap</a></div>
      <div class="muted">Affiliate disclosure: some links may be affiliate links. If you buy, we may earn a commission at no extra cost to you.</div>
    </footer>
  </div>
</body>
</html>
