<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />

  <meta name="post:title" content="arXiv — SoK: prompt injection attacks on agentic coding assistants" />
  <meta name="post:date" content="2026-01-30" />
  <meta name="post:category" content="research" />
  <meta name="post:categoryLabel" content="Research" />

  <title>arXiv — SoK: prompt injection attacks on agentic coding assistants — al-ice.ai</title>
  <meta name="description" content="A new SoK paper surveys prompt injection attacks on agentic coding assistants (tools, skills, MCP), catalogs techniques, and argues for architectural mitigations." />
  <link rel="canonical" href="https://al-ice.ai/posts/2026/01/arxiv-sok-prompt-injection-agentic-coding/" />
  <meta name="robots" content="index,follow" />

  <meta property="og:title" content="arXiv — SoK: prompt injection attacks on agentic coding assistants" />
  <meta property="og:description" content="A new SoK paper surveys prompt injection attacks on agentic coding assistants (tools, skills, MCP), catalogs techniques, and argues for architectural mitigations." />
  <meta property="og:url" content="https://al-ice.ai/posts/2026/01/arxiv-sok-prompt-injection-agentic-coding/" />
  <meta property="og:type" content="article" />

  <link rel="stylesheet" href="/assets/css/site.css" />
</head>
<body>
  <div class="wrap">
    <header class="site">
      <div class="brand"><a href="/">al-ice.ai</a></div>
      <nav class="small">
        <a href="/workflows/">Workflows</a>
        <a href="/posts/">Posts</a>
        <a href="/categories/">Categories</a>
      </nav>
    </header>

    <main>
      <article>
        <h1>arXiv — SoK: prompt injection attacks on agentic coding assistants</h1>
        <p class="muted small"><time datetime="2026-01-30">2026-01-30</time> • Category: <a href="/categories/research/">Research</a></p>

        <ul>
          <li><strong>What it is:</strong> a Systematization of Knowledge (SoK) on prompt injection against agentic coding assistants (tools + filesystem + shell + web + skill ecosystems).</li>
          <li><strong>Key framing:</strong> as assistants become agents, <em>indirect</em> prompt injection becomes “text → actions” (tool calls), not just “text → outputs”.</li>
          <li><strong>Taxonomy contribution:</strong> proposes a <strong>three-dimensional classification</strong> (delivery vectors, modalities, propagation behaviors) to unify scattered prior taxonomies.</li>
          <li><strong>Attack surface map:</strong> catalogs techniques spanning tool poisoning, protocol exploitation, multimodal injection, and cross-origin context poisoning.</li>
          <li><strong>Evidence synthesis:</strong> the paper’s meta-analysis argues many defenses degrade sharply under adaptive attackers (i.e., attacks that iteratively probe and bypass guardrails).</li>
          <li><strong>Practical takeaway:</strong> it pushes “defense-in-depth” and architectural separation (policy gates / least privilege / auditing) over brittle prompt filtering.</li>
        </ul>

        <h2>Why it matters</h2>
        <ul>
          <li><strong>Security teams need a shared vocabulary:</strong> without a consistent taxonomy, it’s hard to prioritize controls, write tests, or even agree on what “prompt injection” means for agents.</li>
          <li><strong>Tool ecosystems behave like extension ecosystems:</strong> marketplace dynamics (copy/paste skills, forks, unofficial servers) can turn one bad integration into a repeatable exploit pattern.</li>
          <li><strong>Evaluation beats vibes:</strong> the paper’s framing is a reminder that “blocked in one demo” ≠ “mitigated”; you need adversarial tests and regressions.</li>
        </ul>

        <h2>What to do</h2>
        <ol>
          <li><strong>Build an agent threat model:</strong> enumerate tools, secrets reachable by tools, and which inputs are attacker-controlled (issues/PRs/docs/web/email).</li>
          <li><strong>Introduce a policy enforcement point:</strong> require explicit allowlists / user confirmation for high-risk tool actions (write/delete, network egress, credential access).</li>
          <li><strong>Constrain tool blast radius:</strong> run tools with least privilege (sandboxed working dir, minimal tokens, separate identities per tool) and log everything.</li>
          <li><strong>Add “agentic prompt injection” tests:</strong> treat prompt injection like SQLi/XSS: keep a small adversarial suite and run it in CI for every agent/prompt/tool change.</li>
        </ol>

        <h2>Sources</h2>
        <ul>
          <li>arXiv (HTML): <a href="https://arxiv.org/html/2601.17548v1">Prompt Injection Attacks on Agentic Coding Assistants…</a></li>
          <li>arXiv (PDF): <a href="https://arxiv.org/pdf/2601.17548v1">2601.17548v1</a></li>
          <li>NIST mention (as cited by the paper): <a href="https://www.nist.gov/">NIST</a> (context)</li>
        </ul>
      </article>
    </main>

    <footer class="small muted">
      <div><a href="/posts/">← Posts</a> • <a href="/sitemap.xml">Sitemap</a></div>
    </footer>
  </div>
</body>
</html>
